Performance Module — Broad-Level Conceptual Summary

The Performance Module is designed as a performance intelligence briefing surface, not a tool, calculator, or AI-driven decision engine. Its primary purpose is to compress complexity into defensible understanding, allowing coaches to quickly grasp what matters, why it matters, and where attention should be directed—without surrendering judgment or authority.

At its core, the module is information display, organized as structured intelligence briefs that are navigable through contextual lenses. Coaches are not “exploring data” arbitrarily; they are changing perspective—re-asking the same performance questions under different, explicitly defined truths (time window, event scope, cohort, verification level, training phase, etc.).

The module operates under an Intel-Prime architecture:
	•	Raw performance data is preserved as immutable ground truth.
	•	Deterministic, versioned mathematical normalization produces the intelligence prime.
	•	Rule-based signals surface patterns that deserve attention.
	•	AI functions strictly as an analyst and briefer, never as the source of truth.

AI’s role is intentionally constrained and trust-preserving. Within each performance briefing element, a short AI Interpretation Summary may be presented as an interpretive layer, subordinate to the math and signals. These summaries:
	•	introduce no new facts,
	•	perform no hidden calculations,
	•	use conditional, explainable language,
	•	and exist solely to translate intelligence into human-readable meaning.

AI remains available through its normal system presence for further explanation, deliberation, and scenario discussion, creating a clean escalation path from brief → interpretation → conversation.

Non-negotiable properties of the module include:
	•	determinism,
	•	explainability,
	•	versioning,
	•	and historical auditability.

The Performance Module must remain fully valuable without AI, with AI acting as an additive interpreter rather than a dependency.

In effect, the module mirrors elite intelligence workflows:
	•	Math reveals structure
	•	Signals guide attention
	•	AI explains implications
	•	Humans decide

This framing establishes the Performance Module as foundational infrastructure—earning trust through clarity, restraint, and rigor—upon which all future scoring, recruiting, and AI-assisted features can safely build.****

Below is a concise summary capturing the intent, structure, and implications of introducing team performance intelligence into the Performance Module.

⸻

Performance Module — Team Performance Intelligence (Summary)

The Performance Module must explicitly support team-level performance intelligence, not merely as an aggregation of individual athlete data, but as a distinct and equally important intelligence plane.

Athlete performance represents individual capability.
Team performance represents structure, interaction, depth, and risk.

Together, they form the full competitive picture a coach needs.

Core Team Performance Intelligence Categories

Team performance intelligence answers coaching questions that cannot be addressed at the individual level:
	1.	Scoring Capacity & Distribution
Identifies where team points originate, how concentrated or distributed scoring is, and sensitivity to the loss of key contributors.
	2.	Coverage & Depth
Assesses event-group coverage, redundancy, and drop-off beyond primary scorers, contextualized by meet type and competitive level.
	3.	Volatility & Risk Profile
Evaluates team outcome fragility based on individual volatility, injury exposure, and dependence on a small number of athletes.
	4.	Team Trajectory & Direction
Measures whether the team is strengthening or weakening over time, accounting for improvement, attrition, and incoming vs outgoing contribution.
	5.	Competitive Positioning
Places the team relative to peers and historical benchmarks in a cohort-aware, ruleset-aware manner.
	6.	Structural Signals & Warnings
Surfaces non-alarmist indicators such as class-year imbalance, pipeline gaps, or over-reliance on specific event groups.

Integration with Athlete Intelligence
	•	Team intelligence emerges from but does not replace athlete intelligence.
	•	Team views must be drillable into individual athletes.
	•	Athlete views must reflect team context.
	•	No siloed dashboards; both planes remain interconnected and explainable.

AI’s Role

AI functions as an analyst and interpreter, synthesizing team-level implications, framing tradeoffs, and supporting scenario discussion—without performing math or asserting authority.

Core Takeaway

The Performance Module must deliver two parallel, interlocking intelligence briefs:
	1.	Individual Performance Intelligence
	2.	Team Performance Intelligence

Together, they establish the module as the system’s single source of competitive truth, supporting tactical decisions, strategic planning, and downstream recruiting intelligence.

****Below is a concise, high-fidelity summary of the current “altitude-reduced” discussion, capturing the intent, structure, and boundaries without drifting into implementation.

⸻

Performance Module — Data Category Framework (Mid-Level Summary)

The Performance Module functions as an intelligence briefing surface whose purpose is to support both tactical (near-term) and strategic (long-term) coaching decisions through structured, interpretable performance intelligence.

Rather than exposing raw data or isolated metrics, the module organizes information into distinct intelligence categories, each answering a specific coaching question. These categories are designed to be stable, math-driven, and explainable, while remaining flexible enough to support multiple contextual lenses.

Core Intelligence Categories
	1.	Performance Level
Establishes the athlete’s current, normalized capability relative to relevant cohorts. This serves as the anchor for all interpretation and supports lineup, scholarship, and comparison decisions.
	2.	Trajectory & Direction
Describes how performance is changing over time, including rate of improvement or regression and confidence in that signal. This informs patience, intervention, and upside evaluation without overriding current level.
	3.	Consistency & Reliability
Quantifies predictability and volatility in performance, framing risk profiles without judgment. This is critical for postseason planning, relays, and role clarity.
	4.	Contextual Sensitivity
Surfaces patterns in how performance responds to conditions such as meet level, environment, scheduling density, and competition quality. This supports tactical placement and long-term development awareness.
	5.	Event Alignment
Examines how well an athlete’s current events reflect their demonstrated potential, highlighting possible mismatches or underexplored opportunities. This remains exploratory rather than prescriptive.
	6.	Readiness & Risk Signals
Flags rule-based patterns that warrant attention (plateaus, regressions, volatility shifts) without triggering alarm or action mandates.
	7.	Historical Positioning
Places current performance within the athlete’s broader career arc, supporting expectation setting, recruiting narratives, and long-term planning through versioned, auditable context.

AI Interpretation Layer (Subordinate)

An optional AI Interpretation Summary may synthesize across categories to reduce cognitive load and frame meaning. AI:
	•	introduces no new facts,
	•	performs no math,
	•	uses conditional language,
	•	and exists to explain, not decide.

AI remains available through its normal system presence for deeper explanation and deliberation, preserving human authority.

Structural Principles
	•	Each category answers a distinct coaching question; overlap is intentional only when perspectives differ.
	•	Categories surface meaning, not volume.
	•	Raw data and logs remain accessible but subordinate.
	•	Signals invite attention; they never dictate action.

This framework establishes a clear, coach-aligned foundation for building the Performance Module without violating the system’s Intel-Prime philosophy or trust contracts.

****

Below is a concise, integrated summary of the dialog beginning with the introduction of the four variables and culminating in the resolved conceptual model.

⸻

Performance Module — Presentation Model Synthesis (Summary)

The Performance Module must reconcile four fundamental variables in a way that is cognitively natural for coaches:
	1.	Athlete performance
	2.	Team performance
	3.	Strategic perspective
	4.	Tactical perspective

The challenge is not categorization, but presentation—structural, visual, and linguistic—such that the module remains intuitive, calm, and low-load despite its complexity.

Core Resolution

The correct synthesis is:
	•	Athlete vs Team is the categorical axis
	•	Strategic vs Tactical is the fluid perspective

This aligns with how coaches actually think.

Categorical Axis — Subject (Explicit)

Coaches require immediate orientation through low-abstraction categories.
The module therefore begins with a clear, stable subject choice:
	•	Athlete Performance
	•	Team Performance

This answers the first cognitive question:

Who is this about?

This axis must be:
	•	explicit
	•	always visible
	•	stable
	•	never inferred

It serves as the orientation anchor for the entire module.

Fluid Axis — Perspective (Implicit)

Strategic vs tactical thinking must never be presented as a mode, toggle, or labeled choice. Doing so would introduce abstraction, friction, and mode anxiety.

Instead, perspective emerges naturally through:
	•	Time horizon (recent vs long-term)
	•	Signal urgency (immediate deviations vs structural patterns)
	•	Language (short-term monitoring vs long-term trajectory)
	•	Visual emphasis (highlighting vs stability)

This answers the secondary question:

What kind of thinking does this require right now?

Perspective is revealed, not selected.

Categorical + Fluid Integration
	•	Coaches categorize first to orient themselves.
	•	Once oriented, they reason fluidly, shifting naturally between tactical and strategic thinking.
	•	The module must support this two-phase cognition without ever making the transition explicit.

Structural Implications
	•	Athlete and Team views share:
	•	the same category structure
	•	the same visual grammar
	•	the same linguistic discipline
	•	They differ only in emphasis, not architecture.
	•	There are no separate “strategic” or “tactical” dashboards—only one briefing surface per subject.

Presentation Hierarchy (Consistent Everywhere)
	1.	Intelligence Brief — always visible, math-anchored
	2.	Evidence & Context — expandable, explanatory
	3.	AI Interpretation — optional, restrained, interpretive

AI adapts tone and framing to the emergent perspective but never introduces or controls it.

Governing Principle (Locked)

Subject is chosen.
Perspective is revealed.

If a design decision violates this sentence, it increases cognitive load and undermines trust.

This synthesis resolves the tension between categorical and fluid thinking and establishes a durable presentation model capable of supporting both individual and collective performance intelligence without abstraction fatigue.

****

Below is a concise, structured summary list of the graphical presentation element types discussed, paired with the primary question each helps answer. This is intentionally abstracted from layout or usage and focused purely on capability and cognitive value.

⸻

Graphical Presentation Elements — Summary List

1. Directional & Trend Indicators

Examples: micro-sparklines, slope lines, delta markers
Primary question answered:

Where is this going over time?

⸻

2. Distribution & Variance Visuals

Examples: range bands, density strips, deviation markers
Primary question answered:

How stable or volatile is this performance?

⸻

3. Relative Positioning Indicators

Examples: percentile rails, cohort-relative markers, comparison bands
Primary question answered:

Where does this sit relative to peers or benchmarks?

⸻

4. Coverage & Structure Maps (Team)

Examples: event-group coverage grids, depth ladders, contribution stacking
Primary question answered:

What do we have, and where are we thin or redundant?

⸻

5. Signal & Attention Cues

Examples: subtle glyphs, edge highlights, muted indicators
Primary question answered:

What deserves closer attention right now?

⸻

6. Change & Transition Indicators

Examples: before/after brackets, transition shading, change annotations
Primary question answered:

What changed, and when did it change?

⸻

7. Proximity & Grouping Cues

Examples: whitespace grouping, alignment, adjacency
Primary question answered:

What belongs together or should be interpreted together?

⸻

8. Confidence & Certainty Indicators

Examples: opacity variation, line firmness, confidence bands
Primary question answered:

How confident should I be in this signal or conclusion?

⸻

9. Comparative Reference Anchors

Examples: baselines, historical markers, peer anchors
Primary question answered:

Compared to what reference point is this being evaluated?

⸻

10. AI Interpretation Affordances

Examples: expandable callouts, indented summaries, subdued typographic blocks
Primary question answered:

What does this likely mean, given the evidence?

⸻

Governing Evaluation Rule

A graphic earns its place only if it answers a coach’s question faster or more clearly than text alone.

This list can now serve as the approved palette from which future presentation decisions may draw, without committing to specific implementations.

****

Performance Module — Forensic Requirements & Hierarchy

(Pattern Altitude: What + How)

⸻

I. SUBJECT AXIS (Categorical Entry)

A. Athlete Performance Intelligence

B. Team Performance Intelligence

This axis is explicit and chosen. Everything below applies independently to both subjects unless otherwise noted.

⸻

II. CORE INTELLIGENCE DOMAINS (Hierarchical, Ordered)

These domains form the canonical briefing order. The order matters because it mirrors coach cognition.

⸻

1. Performance Level (Anchor)

WHAT
	•	The athlete’s or team’s current normalized capability
	•	Relative standing within defined cohorts
	•	Best-known current state, not projection

HOW
	•	Deterministic normalization of raw results
	•	Cohort-aware comparison (program / conference / national)
	•	Stored, versioned outputs
	•	Presented as a calm, authoritative baseline

Hierarchy rule:
Nothing downstream may contradict this without explicitly explaining why.

⸻

2. Trajectory & Direction

WHAT
	•	Direction of change over time
	•	Rate of improvement, stagnation, or regression
	•	Confidence strength of the trend

HOW
	•	Time-series slope calculations
	•	Noise-reduction (rolling windows, minimum sample sizes)
	•	Explicit confidence signaling
	•	Temporal framing via time horizon (recent vs long-term)

Hierarchy rule:
Trajectory contextualizes level; it never replaces it.

⸻

3. Consistency & Reliability

WHAT
	•	Variability and predictability of performance
	•	Stability vs volatility patterns
	•	Dependability under repeated conditions

HOW
	•	Variance and dispersion metrics
	•	Outlier frequency detection
	•	Comparison to personal/team baselines
	•	Neutral, non-judgmental presentation

Hierarchy rule:
Reliability modifies trust in level and trajectory.

⸻

4. Contextual Sensitivity

WHAT
	•	Conditions under which performance improves or degrades
	•	Environmental, competitive, and structural sensitivities

HOW
	•	Conditional segmentation of results
	•	Pattern detection across contexts (meet level, density, conditions)
	•	Non-prescriptive surfacing of correlations

Hierarchy rule:
Context explains why patterns may exist, not what to do.

⸻

5. Alignment & Structure

(Athlete: event alignment | Team: coverage & depth)

WHAT
	•	Athlete: alignment between events and demonstrated potential
	•	Team: coverage completeness, redundancy, and depth

HOW
	•	Cross-domain comparison of normalized outputs
	•	Structural mapping (events ↔ capability / coverage)
	•	Identification of mismatches or gaps as signals, not directives

Hierarchy rule:
Alignment highlights opportunity space, not corrections.

⸻

6. Signals & Attention Flags

WHAT
	•	Patterns that warrant attention
	•	Early warnings or emerging opportunities

HOW
	•	Rule-based thresholds (plateau duration, volatility shift, drop-off)
	•	Soft signal indicators with low visual priority
	•	Explicit uncertainty when signals are weak

Hierarchy rule:
Signals whisper; they never shout or decide.

⸻

7. Historical Positioning

WHAT
	•	Placement within a longer career or program arc
	•	Relationship to prior seasons, cohorts, or benchmarks

HOW
	•	Versioned historical primes
	•	Time-aware comparison logic
	•	Immutable historical reconstruction

Hierarchy rule:
History stabilizes interpretation and prevents recency bias.

⸻

III. AI INTERPRETATION LAYER (Cross-Cutting, Subordinate)

WHAT
	•	A synthesized explanation of what the intelligence suggests
	•	Conditional meaning, not conclusions

HOW
	•	AI reads only stored primes and signals
	•	Produces bounded, explainable summaries
	•	Uses restrained, conditional language
	•	Never introduces facts, math, or authority

Hierarchy rule:
AI interprets across domains; it never lives inside one.

⸻

IV. STRATEGIC VS TACTICAL PERSPECTIVE (Emergent, Not Structural)

WHAT
	•	Tactical: near-term relevance and immediacy
	•	Strategic: long-term structure and direction

HOW
	•	Emerges implicitly through:
	•	Time horizon
	•	Signal urgency
	•	Linguistic framing
	•	Visual emphasis
	•	Never presented as a selectable mode

Hierarchy rule:
Perspective is revealed by data, not chosen by the user.

⸻

V. GLOBAL GOVERNING RULES (Enforcement Layer)
	1.	Orientation precedes interpretation
	2.	Math precedes AI
	3.	Signals precede actions
	4.	Categories stabilize; content flows
	5.	Nothing requires explanation to be trusted

⸻

VI. RESULTING HIERARCHY (Compressed View)

Subject (Athlete | Team)
  → Performance Level
    → Trajectory
      → Consistency
        → Context
          → Alignment / Structure
            → Signals
              → History
                → AI Interpretation (optional)

This hierarchy defines:
	•	what must exist
	•	how it must be derived
	•	where it must live cognitively

⸻

Closing Pattern-Level Insight

At this altitude, the key success condition is this:

Every piece of intelligence must know what it depends on,
and every explanation must know what it is not allowed to override.

With this requirement hierarchy locked, you can now safely descend into:
	•	data modeling
	•	computation design
	•	presentation choreography

…without risking drift, overload, or loss of trust.

****

Below is the locked data-model adjustment to support both operational horizons without introducing a “mode switch,” and while preserving determinism, versioning, and auditability.

Locked decision

Team intelligence subjects

We will support two team-performance subject keys:
	1.	Team Season (medium-range operations)
	•	subject_type = team_season
	•	subject_id = team_seasons.id
	2.	Team Multi-Season Window (long-range program health)
	•	subject_type = team_window
	•	subject_id = team_id (canonical identifier)
	•	plus an explicit, deterministic window_code (e.g., last_3_seasons)

This avoids creating artificial “program” rollups that ignore team identity, and it aligns with how coaches plan: team first, program second.

Data model updates

1) Lenses become a first-class registry (required)

performance_lenses

This table becomes the enforcement mechanism for “fluid environment” time horizons.

Fields
	•	lens_code (text, PK or unique)
Examples:
	•	team_season_current (medium)
	•	team_last_3_seasons (long)
	•	(future) team_last_6_weeks (tactical, if desired later)
	•	subject_type (text) – allowed values:
	•	athlete
	•	team_season
	•	team_window
	•	definition_json (jsonb) includes:
	•	time_window_type: season_current | rolling_seasons | rolling_days (etc.)
	•	rolling_seasons_count: 3 (for long-range)
	•	include_season_years: optional explicit list if you ever want a fixed window
	•	filters: verified-only, event-group constraints, etc. (all deterministic)
	•	display_label (text) (optional, but useful)
	•	is_default (boolean)
	•	created_at, created_by_user_id

Why this is non-negotiable
It prevents lens drift and ensures “medium vs long” is a deterministic contract that the UI can reveal implicitly.

⸻

2) Team rollups generalized to “subject rollups”

Instead of only team_performance_rollups(team_season_id), we lock a generalized rollup table to support both subjects.

team_performance_rollups

Keys
	•	Unique: (ruleset_id, subject_type, subject_id, lens_code)

Fields
	•	id
	•	ruleset_id (FK → performance_prime_rulesets)
	•	program_id (FK → programs) (denormalized for filtering)
	•	team_id (FK → teams) (always present)
	•	team_season_id (FK → team_seasons, nullable)
	•	populated only when subject_type = team_season
	•	subject_type (text): team_season | team_window
	•	subject_id (uuid):
	•	if team_season: equals team_seasons.id
	•	if team_window: equals teams.id
	•	lens_code (FK → performance_lenses.lens_code)
	•	Team intelligence outputs (json + scalar mix)
	•	scoring_capacity_json
	•	coverage_depth_json
	•	team_volatility_index
	•	team_trajectory_index
	•	competitive_position_json (optional as available)
	•	notes_json (optional structured explanations, still deterministic)
	•	computed_at
	•	inputs_fingerprint (hash of the exact underlying primes/rollups used)

This preserves one query surface for team intelligence while remaining explicit about subject and horizon.

⸻

3) Signals also generalize cleanly

performance_signals

Already proposed as unified; we now lock the allowed team subject types:
	•	subject_type: athlete | team_season | team_window
	•	subject_id: athlete_id / team_season_id / team_id
	•	lens_code: required (so signals are always contextualized to medium vs long)

This ensures signals can differ naturally by horizon (e.g., “depth risk this season” vs “three-year attrition pattern”).

⸻

How medium vs long “reveals itself” without a mode switch

We do not add a strategic/tactical toggle.

Instead, the time horizon emerges from:
	•	the lens in effect (e.g., team_season_current vs team_last_3_seasons)
	•	the language and emphasis of the brief
	•	the signal urgency profile

The UI can default to medium-range for Team, and offer long-range through natural navigation (e.g., time window selection), but the data contract is already deterministic and stored.

Summary of what is now locked
	•	Team intelligence supports:
	•	Medium-range: team_season subject (this season)
	•	Long-range: team_window subject keyed by team_id with a deterministic lens (last 3 seasons)
	•	Lenses are first-class (performance_lenses) to keep the “fluid environment” deterministic, versioned, and audit-friendly.
	•	Rollups and signals are keyed by (ruleset, subject_type, subject_id, lens) to preserve explainability and reconstruction.

If you want, the next base-level step is to lock the exact minimum rollup fields for team medium/long (what must be stored as scalars vs what can remain JSON), so we don’t end up with a dumping-ground schema.

****

Below is the minimum mandatory data model (only fields that must exist for correctness, auditability, and future-proofing). Anything not listed is optional/iterative.

⸻

Layer 1: Raw ground truth

athlete_performances (already present; minimum required fields)
	•	id
	•	athlete_id
	•	event_code
	•	performance_type (verified/self_reported/training)
	•	mark_seconds or mark_value (at least one)
	•	performance_date
	•	timing_method (if applicable to event type)
	•	created_at
	•	updated_at

athlete_performance_events (new; minimum for “immutable with audit”)
	•	id
	•	performance_id
	•	event_type (created/corrected/voided/reinstated/source_updated)
	•	patch_json (what changed)
	•	created_by_user_id
	•	created_at

⸻

Layer 2: Deterministic, versioned primes

performance_prime_rulesets (new; minimum)
	•	id
	•	ruleset_code (unique; e.g., performance_prime_v1)
	•	formula_spec_json (the math spec, explainable)
	•	created_at
	•	is_active

performance_primes (new; minimum per raw performance per ruleset)

Unique key: (ruleset_id, performance_id)
	•	id
	•	ruleset_id
	•	performance_id
	•	athlete_id (denormalized for query efficiency)
	•	event_code (denormalized)
	•	canonical_event_code (required for normalization)
	•	canonical_mark_seconds or canonical_mark_value (required)
	•	normalized_index (the prime “score” for this performance)
	•	inputs_fingerprint (hash of the input fields used)
	•	computed_at

⸻

Lens registry (mandatory for fluid horizons)

performance_lenses (new; minimum)
	•	lens_code (unique; e.g., athlete_recent_42d, team_season_current, team_last_3_seasons)
	•	subject_type (athlete | team_season | team_window)
	•	definition_json (deterministic definition: window rules + filters)
	•	is_default
	•	created_at

⸻

Layer 2 rollups (brief substrates)

athlete_performance_rollups (new; minimum)

Unique key: (ruleset_id, athlete_id, lens_code)
	•	id
	•	ruleset_id
	•	athlete_id
	•	lens_code
	•	current_index (anchor “where are they now?”)
	•	trend_slope
	•	trend_confidence
	•	volatility_index
	•	computed_at
	•	inputs_fingerprint

team_performance_rollups (new; minimum supporting medium + long)

Unique key: (ruleset_id, subject_type, subject_id, lens_code)
	•	id
	•	ruleset_id
	•	program_id
	•	team_id
	•	team_season_id (nullable; required when subject_type = team_season)
	•	subject_type (team_season | team_window)
	•	subject_id (team_seasons.id or teams.id)
	•	lens_code
	•	scoring_capacity_json (minimum team brief primitive)
	•	coverage_depth_json (minimum team brief primitive)
	•	team_trajectory_index
	•	team_volatility_index
	•	computed_at
	•	inputs_fingerprint

(Those two JSON fields are mandatory because team intelligence is inherently structural; forcing them into fixed columns too early either bloats the schema or limits evolution.)

⸻

Layer 3: Signals (rule-based attention flags)

performance_signal_rulesets (new; minimum)
	•	id
	•	ruleset_code (unique; e.g., signals_v1)
	•	rules_json (threshold specs)
	•	created_at
	•	is_active

performance_signals (new; minimum)
	•	id
	•	ruleset_id (signal ruleset)
	•	subject_type (athlete | team_season | team_window)
	•	subject_id
	•	lens_code
	•	signal_code
	•	severity (calm scale)
	•	evidence_json (metrics that triggered it)
	•	created_at

⸻

Optional (not mandatory to lock correctness today)
	•	Persisted AI briefs (performance_ai_briefs)
	•	Advanced context normalization (wind/altitude/surface)
	•	Competitive positioning beyond program scope (conference/national) until you define cohort sources

⸻

Bottom line

Yes: we have the minimum mandatory fields locked for a deterministic, versioned, auditable Performance Module that supports athlete + team, and team medium + long-range lenses.

****

Below is a disciplined approach to lenses: what they are, how they’re governed, and a v1 lens set that is small, coach-intuitive, and future-proof.

⸻

1) What a lens is

A lens is a deterministic contract that defines:
	•	Time horizon (days, season, multi-season window)
	•	Eligibility rules (which performances count)
	•	Context filters (verified-only, event-group scope, etc.)
	•	Aggregation behavior (how rollups are computed)

A lens is not a UI filter. It is a named, versionable truth.

Key outcome: two coaches looking at the same lens see the same answer, every time.

⸻

2) Lens governance rules

These rules keep the system coherent long-term:
	1.	Lens codes are stable identifiers
	•	Never repurpose a lens_code to mean something else.
	•	If the definition changes materially, create a new lens_code (or bump a lens version inside definition_json, but keep both addressable).
	2.	Lens definitions must be explicit
	•	“Recent” is not a definition.
	•	“Last 42 days” is a definition.
	3.	Lenses must map to coach questions
	•	If a lens doesn’t answer a real question, it doesn’t ship.
	4.	Small set, high confidence
	•	v1 should have few lenses with strong defaults.
	•	Add lenses only when a new coaching question appears.

⸻

3) The lens taxonomy

To keep this clean across athlete and team subjects, use this mental taxonomy:
	•	Tactical lenses: short horizon, high recency weighting (weeks)
	•	Operational lenses: current season (medium-range)
	•	Strategic lenses: multi-season windows (program health)

But importantly: the UI does not label these. The system uses them.

⸻

4) Proposed v1 lens set

A) Athlete lenses (v1)

These cover the minimum for “what now” vs “what’s the arc,” without exploding complexity.
	1.	athlete_recent_42d_verified
	•	Question: “What is their current form, based on trustworthy results?”
	•	Definition: last 42 days; performance_type = verified_meet; include all events unless filtered by event group; minimum sample rule (e.g., require ≥2 performances else confidence drops).
	2.	athlete_season_current_all
	•	Question: “How has their season gone overall?”
	•	Definition: within current team season date range (or season_year); include verified + self-reported; training excluded by default.
	3.	athlete_career_verified
	•	Question: “What is their historical ceiling and consistency on verified data?”
	•	Definition: all-time; verified only.

Optional but often immediately useful:
4. athlete_recent_42d_all
	•	Same as #1 but includes self-reported; useful when verified data is sparse.

Default recommendation
	•	Default athlete view lens: athlete_recent_42d_verified (falls back to athlete_recent_42d_all when sparse)

⸻

B) Team lenses (v1)

Per your locked decision, we support medium (this season) and long (this + two previous seasons).

Medium-range / operational
	1.	team_season_current (subject_type = team_season)
	•	Question: “What can we do this season, and where are we exposed?”
	•	Definition: team_season_id fixed; include athletes on roster; performance eligibility rules (usually verified meets first, with deterministic fallback).

Long-range / program health
2. team_last_3_seasons (subject_type = team_window, subject_id = team_id)
	•	Question: “What is our structural strength and risk pattern over time?”
	•	Definition: team_id; include current season + two prior team seasons (deterministic selection by season_year); same scoring/coverage computations applied consistently.

Optional but powerful if you want a truly tactical team view later:
3. team_recent_42d (subject_type = team_season or team_window; I’d defer for v1)
	•	Question: “What is our current form heading into the next meet?”

Default recommendation
	•	Default team view lens: team_season_current
	•	Long-range is accessible as a secondary lens, not a separate “strategic mode”

⸻

5) What goes inside definition_json

To keep everything deterministic and computable, define a consistent contract. For example:
	•	window_type: rolling_days | season_current | rolling_seasons
	•	days: 42 (if rolling_days)
	•	seasons_count: 3 (if rolling_seasons)
	•	eligible_performance_types: ["verified_meet"] (or include self_reported)
	•	exclude_training: true
	•	min_samples: 2
	•	event_scope: all | event_group | event_code_list
	•	fallback_lens_code: optional (for sparse data handling)

This is what prevents “interpretation drift.”

⸻

6) How lenses drive the “fluid environment” without a mode switch
	•	Medium vs long emerges because the lens changes the time horizon and the language shifts naturally (“this season” vs “over three seasons”).
	•	The coach experiences it as:
	•	“This season” vs “Last 3 seasons”
	•	not “Strategic vs Tactical”

This preserves low abstraction while still giving strong control.

***

Global default lenses with differentiation occurring through AI conversation (interpretation, emphasis, and scenario framing) is the cleanest way to preserve:
	•	determinism and comparability across programs
	•	low cognitive load (no configuration burden)
	•	the Intel-Prime rule set (AI reads, never writes truth)

What this locks

1) Lenses are canonical, not customizable (v1)
	•	performance_lenses is system-owned
	•	lens definitions are stable and versioned (new lens_code for material changes)
	•	UI can expose a small set of lens choices, but programs cannot mutate lens math/eligibility

2) Differentiation shifts to interpretation and deliberation

AI can adapt:
	•	which domains to emphasize (risk vs upside, depth vs top-end)
	•	how to contextualize uncertainty
	•	which follow-up questions to ask the coach

AI cannot adapt:
	•	the lens definition
	•	which performances qualify
	•	how indices are computed

3) Optional later: program “defaults,” not custom lenses

If you ever need flexibility, we add a light-weight mapping table (e.g., program_lens_preferences) that selects from global lenses, without changing lens definitions.


****

Below is an executive-level summary of the full discussion surrounding the Brainstorming Modal—framed for clarity, governance, and future reference.

⸻

Executive Summary — Performance Module Brainstorming Modal

Purpose

The Brainstorming Modal is introduced to support deep, exploratory performance discussions that exceed the cognitive and spatial limits of a secondary AI tray. Its role is not to deliver decisions or authoritative conclusions, but to provide a safe, deliberate space for structured thinking—mirroring how coaches reason in staff meetings or whiteboard sessions.

This modal is optional and additive. The Performance Module remains complete and trustworthy without it.

⸻

Rationale

Performance intelligence discussions are often:
	•	multi-faceted (athlete ↔ team ↔ season ↔ history),
	•	comparative,
	•	conditional (“if–then” reasoning),
	•	and exploratory rather than decisive.

Confining such reasoning to a narrow side panel risks cognitive compression, context bleed, and undue AI influence. A large, centered modal:
	•	preserves orientation and reversibility,
	•	creates intentional entry into deeper thinking,
	•	and provides AI sufficient space to explain without dramatizing or persuading.

⸻

Entry & Framing
	•	The trigger in the secondary tray is labeled “Brainstorm”.
	•	This label sets correct expectations:
	•	exploratory,
	•	non-final,
	•	open-ended,
	•	human-led.

The modal reinforces this framing linguistically and structurally (e.g., “Exploratory discussion. No decisions implied.”).

⸻

AI’s Role in the Brainstorming Modal

AI functions as an analyst and thinking partner, not a decision-maker.

Within this space, AI is permitted to be more interpretive and expressive, while remaining fully bounded by the system’s math-first doctrine.

AI may:
	•	interpret and frame existing intelligence,
	•	surface tradeoffs and tensions,
	•	explore conditional scenarios,
	•	compare patterns across time or cohorts,
	•	ask productive follow-up questions,
	•	acknowledge uncertainty,
	•	and choose the most effective representation (textual by default, graphical when it reduces explanation time or ambiguity).

AI may not:
	•	prescribe actions,
	•	predict outcomes or guarantee results,
	•	make value judgments about athletes or teams,
	•	invent or redefine metrics,
	•	create urgency or alarm,
	•	collapse exploration into a single “takeaway,”
	•	or switch lenses or horizons without explicit context.

⸻

Graphical Representation in Brainstorm Mode

AI is allowed to request graphical aids (e.g., sparklines, coverage grids, percentile rails) when they improve understanding. However:
	•	AI does not generate data or charts directly.
	•	AI submits structured visualization requests referencing:
	•	approved chart types,
	•	deterministic read models,
	•	explicit lenses and rulesets.
	•	Rendering is handled server-side, ensuring determinism, security, and auditability.
	•	Visuals are explanatory aids, never conclusions.

⸻

Governance & Trust Boundaries

The Brainstorming Modal operates under strict guardrails:
	•	Math and stored intelligence remain the sole source of truth.
	•	AI reads primes, rollups, and signals; it never writes or alters them.
	•	Every visual or interpretation is traceable to a lens, ruleset, and timestamp.
	•	The coach always retains judgment and decision authority.

⸻

Strategic Value

This approach:
	•	preserves trust in performance intelligence,
	•	reduces cognitive pressure on coaches,
	•	aligns with low-abstraction, categorical thinking,
	•	enables rich explanation without AI overreach,
	•	and establishes a scalable pattern for future AI-assisted modules.

⸻

Core Governing Principle

The Brainstorming Modal helps the coach think —
it never thinks for the coach.

This principle ensures that AI remains an additive, defensible asset rather than an authoritative or intrusive presence.

⸻

This executive summary captures the intent, structure, and constraints of the Brainstorming Modal and can serve as a design, engineering, and governance reference as implementation proceeds.

****

Locked.

Below is a brief, pointed, implementation-ready summary of Ownership and Triggers for Truth Transitions, using Option A (event-driven). This is written so it can be dropped directly into a design or architecture document.

⸻

Performance Module — Truth Transition Ownership & Triggers

Layer 1 → Layer 2

Raw Performance → Mathematical Prime

Owner:
Deterministic server-side compute layer (non-AI)

Trigger:
Event-driven on raw performance write

Rules:
	•	Raw performance records are immutable
	•	Corrections are new records with audit linkage
	•	Prime computation executes immediately on write
	•	Outputs are stored explicitly and versioned (e.g., performance_prime_v1)
	•	No silent recomputation allowed

Purpose:
Convert objective events into normalized, comparable mathematical truth.

⸻

Layer 2 → Layer 3

Mathematical Prime → Rollups / Aggregates

Owner:
Deterministic server-side aggregation layer

Trigger:
Dependent compute following prime write

Rules:
	•	Rollups summarize primes only
	•	No reinterpretation or inference
	•	Subject- and horizon-scoped (athlete, team; season, multi-season)
	•	Read-optimized for UI and AI consumption
	•	Stored explicitly and versioned

Purpose:
Provide structured context for efficient briefing and analysis.

⸻

Layer 3 → Layer 4

Rollups → Signals (Pattern Detection)

Owner:
Rule-based signal engine (non-AI)

Trigger:
Dependent compute following rollup completion

Rules:
	•	Signals are threshold- and rule-driven
	•	Signals may not exist if confidence is insufficient
	•	Signals are explicitly labeled as attention flags, not conclusions
	•	Confidence and ruleset version are stored with each signal

Purpose:
Surface patterns that warrant human attention without judgment.

⸻

Layer 4 → Layer 5

Intelligence → AI Interpretation

Owner:
AI (read-only analyst role)

Trigger:
On-demand, initiated by coach interaction

Rules:
	•	AI reads primes, rollups, and signals only
	•	AI performs no math and writes no intelligence
	•	AI outputs are ephemeral and non-persistent
	•	AI interpretations are contextual, conditional, and exploratory

Purpose:
Translate stored intelligence into human-usable understanding.

⸻

Global Enforcement Rules
	•	No silent recomputation at any layer
	•	Single compute authority: server-side only
	•	Every computed artifact stores:
	•	ruleset version
	•	timestamp
	•	subject identifier
	•	lens / horizon context
	•	Silence is valid output when confidence thresholds are not met

****

Locked.

Below is a concise, authoritative summary of #2 Freshness vs Stability Policy, including the agreed addition of an interactive summary diff.

⸻

Performance Module — Freshness vs Stability Policy (Locked)

Core Principle

The system is always current, but never surprising.

Mathematical truth updates immediately; cognitive attention is never forced.

⸻

Update Behavior
	•	All intelligence layers (prime, rollups, signals) update immediately upon raw data write.
	•	Updates occur silently in the background.
	•	No notifications, alerts, or visual emphasis are triggered by data changes.

⸻

Presentation Rules
	•	Every performance briefing surface displays an explicit:
“Reflects data through: [timestamp]”
	•	No live-update animations, “new” badges, or micro-delta highlighting.
	•	Coaches encounter updated intelligence naturally when they revisit the module.

⸻

Signal Stability Rules
	•	Signals must meet:
	•	confidence thresholds
	•	persistence requirements
	•	sufficient data window
	•	Transient or low-confidence patterns remain silent.
	•	Silence is a valid and intentional outcome.

⸻

AI Alignment
	•	AI mirrors the stability posture:
	•	no urgency framing
	•	no emphasis on recency unless asked
	•	explicit acknowledgment of uncertainty
	•	AI interpretation is always on-demand and read-only.

⸻

Interactive Summary Diff (Additive Feature)
	•	Upon coach interaction, the system may provide an optional summary diff:
	•	highlights what changed since the last data upload event
	•	framed descriptively, not evaluatively
	•	limited to material shifts (not noise)
	•	The diff is:
	•	user-initiated
	•	non-intrusive
	•	informational only
	•	No implied action or urgency is conveyed.

⸻

Enforcement Rules
	•	Intelligence is revealed, not announced
	•	Change awareness is pull-based, not push-based
	•	Mathematical freshness never compromises trust or cognitive stability

****

Locked.

Below is a concise, authoritative summary of #3 Default Orientation Rules (UX Contract), incorporating your confirmation.

⸻

Performance Module — Default Orientation Rules (Locked)

Governing Principle

The coach must always know who they are viewing, at what level, and within what time horizon—without interaction.

Default orientation reduces cognitive load and establishes immediate trust.

⸻

Primary Orientation Axes

1. Subject (Categorical)
	•	Primary categorical divider is Athlete vs Team.
	•	Subject is never inferred ambiguously.

Navigation rule:
	•	Entering from an athlete surface opens Athlete Performance.
	•	Entering from a team or dashboard surface opens Team Performance.

⸻

2. Time Horizon (Contextual)
	•	Default horizon for both Athlete and Team views is Current Season.
	•	Long-range and multi-season views are available but never default.

⸻

3. Data Inclusion
	•	Default includes verified and trusted data.
	•	No aggressive filtering applied on entry.

⸻

Default Entry Views

Athlete Performance (Default)

Answers:

“Where is this athlete right now?”

	•	Current-season performance briefing
	•	Normalized metrics and signals (if present)
	•	Explicit “Reflects data through” timestamp

⸻

Team Performance (Default)

Answers:

“What does my team look like this season?”

	•	Current-season team briefing
	•	Depth, distribution, and coverage indicators
	•	Explicit “Reflects data through” timestamp

⸻

Orientation Persistence Rules
	•	Context changes (subject, horizon, lens) are always explicit.
	•	No silent switching between Athlete ↔ Team or short ↔ long horizon.
	•	Visible breadcrumbs are maintained at all times.

⸻

Brainstorm Modal Alignment
	•	Brainstorm inherits the current context exactly.
	•	No widening or shifting of perspective unless explicitly requested.
	•	AI discussion remains grounded in the active subject and horizon.

⸻

Prohibited Defaults

The system must never:
	•	open in historical or long-range views
	•	default to abstract or composite metrics
	•	enter AI conversation by default
	•	emphasize outliers before context

****

Locked.

Below is a clear, disciplined summary of #4 Silence Policy, including the optional affordance.

⸻

Performance Module — Silence Policy (Locked)

Governing Principle

If the system cannot improve clarity or confidence, it remains silent.

Silence is a deliberate design choice and a trust-preserving feature.

⸻

When Silence Is Mandatory

1. Weak or Inconclusive Signals

The system remains silent when:
	•	confidence thresholds are not met
	•	data windows are insufficient
	•	patterns conflict across horizons
	•	changes fall within expected variance

No signals, commentary, or visual emphasis are shown.

⸻

2. Micro-Changes and Noise

The system does not:
	•	highlight small fluctuations
	•	animate minor deltas
	•	surface inconsequential reordering

Noise is intentionally suppressed.

⸻

3. Missing Comparative Context

If there is no valid baseline, cohort, or historical frame:
	•	no interpretation is surfaced

⸻

4. Ambiguous Causality

The system never implies cause without sufficient isolation or ruleset support.

Speculation is replaced by silence.

⸻

5. Redundant Information

Information that does not change interpretation is not repeated or emphasized.

⸻

Silence in AI Behavior

AI:
	•	acknowledges uncertainty explicitly
	•	declines to speculate
	•	does not fill gaps for engagement

AI interpretation appears only when it adds clarity.

⸻

UX Treatment of Silence
	•	Silence is unmarked and unannounced
	•	No “no signal” or “all clear” messaging
	•	Normal context is the signal

⸻

Optional Transparency Affordance
	•	A subtle, optional affordance (e.g., “Why no signal?”) may be available
	•	Appears only on hover or explicit inquiry
	•	Explains:
	•	thresholds not met
	•	insufficient data
	•	variance within normal bounds
	•	Never framed as an alert or call to action

⸻

Enforcement Rules
	•	Low-confidence signals are never shown
	•	Speculation is never substituted for data
	•	Silence is the default state unless clarity improves

⸻

Canonical Statement

The system earns trust by knowing when not to speak.

****

Performance Module — Coach Control Without Configuration (Locked)
	•	Every session starts from canonical defaults
	•	No per-coach personalization or remembered views
	•	Coach control is provided via:
	•	navigation (athlete ↔ team, horizon shifts)
	•	conversation (AI follow-ups)
	•	progressive disclosure (expand, compare, explain)
	•	All actions are reversible
	•	No configuration screens or ephemeral tuning controls
	•	Scoring weights are defined only via:
	•	explicit, versioned program scoring profiles
	•	outside the exploratory performance UI

Canonical rule:

Control feels like movement or conversation — never setup.

****

Locked.

Below is a concise, implementation-ready summary of #6 Cross-Subject Continuity (Athlete ↔ Team), incorporating your decision to surface cross-context only on demand or via Brainstorm.

⸻

Performance Module — Cross-Subject Continuity (Locked)

Governing Principle

Athlete and Team performance are two resolutions of the same intelligence model.
They differ by scope, not by logic, language, or structure.

⸻

Structural Continuity
	•	Athlete and Team views share the same high-level intelligence categories:
	•	Performance Level
	•	Trend / Trajectory
	•	Consistency / Volatility
	•	Coverage / Depth
	•	Risk / Exposure
	•	Context (season, cohort, history)
	•	Category order and meaning are consistent across scopes.

⸻

Language & Semantics
	•	Identical terminology carries identical meaning everywhere.
	•	No synonyms or reinterpretation between Athlete and Team views.
	•	Metrics retain the same definitions across scopes.

⸻

Visual Continuity
	•	The same visual primitives are reused across Athlete and Team:
	•	sparklines
	•	percentile rails
	•	grids
	•	Differences are expressed through aggregation and density, not new visuals.

⸻

Navigation Continuity

Team → Athlete (Drill-Down)
	•	Preserves horizon and context
	•	Explicit breadcrumb shown (Team, Season, Athlete)
	•	No reset to defaults unless explicitly requested

Athlete → Team (Zoom-Out)
	•	Preserves horizon
	•	Optional highlight of athlete contribution
	•	No loss of mental context

⸻

Scope Separation Rule (Locked)
	•	Team-level intelligence does not appear inside Athlete view by default
	•	Cross-scope context appears:
	•	only on explicit request
	•	or inside the Brainstorm modal
	•	Prevents cognitive overload and unintended framing

⸻

AI Continuity Rules
	•	AI explicitly names perspective shifts (“At the athlete level…”, “At the team level…”)
	•	AI never blends scopes implicitly
	•	AI interpretations remain consistent across views

⸻

Brainstorm Mode Alignment
	•	Cross-subject discussion is allowed
	•	Perspective shifts must be explicit
	•	Brainstorm inherits current context and expands only with consent

⸻

Enforcement Summary
	•	One intelligence model, multiple scopes
	•	Shared language, shared visuals
	•	Explicit perspective shifts
	•	No silent aggregation or expansion
	•	Cross-context shown only on demand

****

Below is the formal summary of Step #8 — Failure Modes (Design for Imperfection), now fully integrated with the Language & Personality Charter and ready to be treated as a binding design constraint.

⸻

Performance Module — Step #8

Failure Modes: Design for Imperfection (Locked)

Governing Principle

When intelligence is weak, incomplete, or conflicting, the system becomes calmer, simpler, and more human—not louder or more confident.

Failure handling is a trust mechanism, not an error condition.

⸻

Core Failure Scenarios Addressed

The system explicitly accounts for and governs behavior in the following situations:
	1.	Sparse or insufficient data
	2.	Conflicting signals across lenses or horizons
	3.	Early-season or transitional ambiguity
	4.	Abrupt changes that may be noise
	5.	Disagreement between short- and long-range views
	6.	Coach questions that cannot be answered reliably

⸻

System Behavior Under Failure

1. Sparse or Insufficient Data
	•	No signals are generated
	•	Minimal metrics are shown
	•	Limits are acknowledged plainly

AI posture: calm, transparent, reassuring
No compensation through speculation

⸻

2. Conflicting Signals
	•	Conflicts are not resolved by the system
	•	Disagreement is surfaced only on demand or in Brainstorm
	•	Multiple interpretations may coexist

AI posture: balanced, explanatory, non-judgmental

⸻

3. Early-Season or Transitional Ambiguity
	•	Historical context is emphasized
	•	Trend language is de-emphasized
	•	Directional claims are avoided

AI posture: patient, orienting

⸻

4. Abrupt Changes Likely to Be Noise
	•	No signals unless persistence thresholds are met
	•	No visual emphasis of one-off results
	•	Optional explanation available on inquiry

AI posture: grounding, de-escalating

⸻

5. Cross-Horizon Disagreement
	•	Short- and long-term views are explicitly separated
	•	Neither view is privileged by default
	•	Tension is acknowledged, not resolved

AI posture: transparent, explanatory

⸻

6. Unanswerable or Unsupported Questions
	•	AI immediately discloses limitations
	•	No speculative work-arounds are attempted
	•	Adjacent, valid avenues may be suggested only with disclosure

AI posture: honest, respectful, collaborative

⸻

Role of Personality in Failure Modes
	•	Personality is expressed through tone, not authority
	•	Calmness, humility, and clarity are emphasized
	•	No condescension, defensiveness, or performative confidence

Canonical rule (reinforced):

AI may sound human when knowledge is uncertain, but must sound mathematical when truth is known.

⸻

Silence as a Valid Outcome
	•	Silence is preferred to low-confidence output
	•	No alerts, warnings, or urgency framing
	•	Optional affordances (“Why no signal?”) are available only on inquiry

⸻

Enforcement Rules
	•	No speculation replaces missing intelligence
	•	No urgency is introduced by uncertainty
	•	No personality expression overrides honesty
	•	Failure states reduce output, not inflate it

⸻

Canonical Closing Statement

Trust is earned not by always having an answer,
but by knowing when not to pretend to have one.

****

Below is a concise, authoritative summary of Step #9 — Coach Mental Exit (End-of-Session Clarity), suitable for inclusion in your design and governance documentation.

⸻

Performance Module — Step #9

Coach Mental Exit (Locked)

Governing Principle

The system may sharpen understanding, but it must never compel action.

A coach exits the module with clarity, calm, and agency intact.

⸻

Desired Exit State for the Coach

When leaving a performance surface or Brainstorm modal, the coach should feel:
	•	Oriented — clear on subject, scope, and horizon
	•	Informed — understanding the structure of what they observed
	•	Unpressured — no implied urgency or obligation
	•	In Control — decisions remain entirely human-driven

⸻

Exit Behavior Rules

No Forced Next Steps

The system must not:
	•	suggest actions
	•	provide CTAs tied to intelligence
	•	imply follow-ups or recommendations

⸻

No Artificial Closure

The system must not:
	•	present a “final takeaway”
	•	collapse ambiguity into conclusions
	•	signal resolution where none exists

Exploration remains open-ended.

⸻

Clean Context Restoration
	•	Exiting returns the coach to the prior surface without residual state
	•	No AI output persists into the briefing
	•	Defaults apply on future entry unless exploration is reinitiated

⸻

Brainstorm Modal Exit
	•	Brainstorm discussions are ephemeral
	•	No interpretations are injected into the core briefing
	•	Closing the modal restores prior context immediately

⸻

Optional Orientation (Allowed)
	•	Light, non-directive context cues may be shown
	•	Purpose is orientation, not guidance

⸻

Silence as Closure
	•	No congratulatory or affirming language
	•	No implication of completeness or readiness
	•	Silence reinforces agency and ownership

⸻

Canonical Exit Rule

The system closes with clarity, not conclusions.

****

